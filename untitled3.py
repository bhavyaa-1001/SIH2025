# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OyjxKX4duGPrYiKweh7US103EoTpeFIV
"""

# pip install optuna xgboost scikit-learn pandas openpyxl

import optuna
import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error
import numpy as np

data = pd.read_excel('/content/Meta_data_without_sg_cleaned_final.xlsx')

display(data)

# Drop the 'dg' column as requested in a separate cell
if 'dg' in data.columns:
    data = data.drop(columns=['dg'])
    print("Dropped 'dg' column.")
else:
    print("'dg' column not found in the DataFrame.")

# Display the first few rows to confirm
display(data.head())

# Drop columns that are not in the desired output format (Code was already dropped, dg is dropped in a separate cell)
# Check if 'Code' exists before dropping to avoid errors
data = data.drop(columns=[col for col in ['Code'] if col in data.columns])


# Define the custom texture class encoding mapping - UPDATED
custom_texture_encoding = {
    "SANDY LOAMY": 6, # Note: There was a typo, should be SANDY LOAM, updated below
    "SANDY CLAY": 5,
    "LOAM": 2,
    "CLAY LOAM": 1,
    "CLAY": 0,
    "SILTY LOAM": 9,
    "LOAMY SAND": 3,
    "SILTY CLAY LOAM": 8,
    "SILTY CLAY": 7,
    "SAND": 4,
    "SANDY LOAM": 10, # Added encoding for SANDY LOAM
    "SANDY CLAY LOAM": 11, # Added encoding for SANDY CLAY LOAM
    "Unknown": -1 # Keep Unknown for any other unexpected types
}

# Check if 'Texture Class' exists and rename it to 'Texture'
if 'Texture Class' in data.columns:
    data = data.rename(columns={'Texture Class': 'Texture'})

# Apply the encoding to the 'Texture' column if it exists
if 'Texture' in data.columns:
    data['Texture Encoded'] = data['Texture'].map(custom_texture_encoding).fillna(custom_texture_encoding["Unknown"])
    # Drop the original 'Texture' column after encoding
    data = data.drop(columns=['Texture'])
else:
    print("Warning: 'Texture Class' or 'Texture' column not found in the DataFrame. Cannot perform texture encoding.")
    # If texture column is missing, create 'Texture Encoded' with default unknown values
    data['Texture Encoded'] = custom_texture_encoding["Unknown"]


# Define the desired column order, now including the encoded texture and excluding the original texture
desired_column_order = ['Clay', 'Silt', 'Sand', 'Texture Encoded', 'OC', 'Ksat']


# Reindex the DataFrame to the desired column order
# This will also drop any columns not in the desired_column_order
# Use 'Texture Encoded' in the desired order
data = data.reindex(columns=desired_column_order)


# Separate features (X) and target (y)
# Ensure 'Ksat' is in data.columns before dropping
if 'Ksat' in data.columns:
    X = data.drop(columns=['Ksat'])
    y = data['Ksat']
else:
    print("Error: 'Ksat' column not found in the DataFrame. Cannot separate features and target.")
    X = data # Assign all remaining columns to X if Ksat is missing
    y = pd.Series(dtype='float64') # Create an empty Series for y


# Display the first few rows of the modified data to verify
display(data.head())

# Save the full data DataFrame to an Excel file
output_filename = 'full_data.xlsx'
data.to_excel(output_filename, index=False) # index=False prevents writing the DataFrame index as a column

print(f"Full data saved to {output_filename}")



# Assuming X and y are already prepared with the correct columns and cleaning
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

def objective(trial):
    param = {
        "verbosity": 0,
        "objective": "reg:squarederror",
        "eval_metric": "rmse",
        "booster": "gbtree",
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "reg_alpha": trial.suggest_float("reg_alpha", 0, 5),
        "reg_lambda": trial.suggest_float("reg_lambda", 0, 5),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
    }

    model = xgb.XGBRegressor(**param, random_state=42, n_jobs=-1)

    # The explicit casting is removed as the data cleaning is handled before train-test split.
    # X_train['OC'] = X_train['OC'].astype(float)
    # X_train['Db'] = X_train['Db'].astype(float)

    # Train the model
    model.fit(X_train, y_train)

    # Evaluate the model using cross-validation
    score = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')
    rmse = -score.mean() # Optuna minimizes the objective function, so we use negative RMSE

    return rmse

# The previous cleaning steps are now integrated into the cell where features and target are defined.
# We will keep this cell here but it might not be necessary depending on the data loading and initial cleaning.
# If further cleaning is needed after the initial loading and column selection, it can be added here.

# Example: If you need to convert 'OC' to numeric again after reindexing
# X['OC'] = pd.to_numeric(X['OC'], errors='coerce')
# X = X.dropna(subset=['OC']) # Only drop rows where 'OC' is missing after conversion
# y = y[X.index] # Filter y to match the index of the cleaned X

# Based on the user's request, I've removed 'Db' from the desired output and cleaning steps.
# If 'OC' still needs cleaning, add it here. Assuming it's handled during initial loading or reindexing.

# Let's ensure the 'OC' column in X is numeric, handling potential errors
X['OC'] = pd.to_numeric(X['OC'], errors='coerce')

# Drop rows where 'OC' could not be converted to numeric
X = X.dropna(subset=['OC'])

# Ensure y aligns with the cleaned X
y = y[X.index]

X.info()

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=100, show_progress_bar=True)

print("Best trial:")
print(study.best_trial.params)

best_params = study.best_trial.params
final_model = xgb.XGBRegressor(**best_params, random_state=42, n_jobs=-1)
final_model.fit(X_train, y_train)

y_pred = final_model.predict(X_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Test RMSE: {rmse_test}")

y_min = y.min()
y_max = y.max()
y_mean = y.mean()
y_std = y.std()

print("Min:", y_min, " Max:", y_max, " Mean:", y_mean, " Std:", y_std)

rmse = 6.5931851139978725

# Normalized RMSE (relative to range)
nrmse_range = rmse / (y_max - y_min)

# Normalized RMSE (relative to mean)
nrmse_mean = rmse / y_mean

print("NRMSE (range):", nrmse_range)
print("NRMSE (mean):", nrmse_mean)

from sklearn.preprocessing import LabelEncoder

# Define the custom texture class encoding mapping
custom_texture_encoding = {
    "SANDY LOAMY": 6, # Note: There was a typo, should be SANDY LOAM, updated below
    "SANDY CLAY": 5,
    "LOAM": 2,
    "CLAY LOAM": 1,
    "CLAY": 0,
    "SILTY LOAM": 9,
    "LOAMY SAND": 3,
    "SILTY CLAY LOAM": 8,
    "SILTY CLAY": 7,
    "SAND": 4,
    "SANDY LOAM": 10, # Added encoding for SANDY LOAM
    "SANDY CLAY LOAM": 11, # Added encoding for SANDY CLAY LOAM
    "Unknown": -1 # Keep Unknown for any other unexpected types
}

def classify_soil_texture(sand_pct, silt_pct, clay_pct):
    """
    Classifies soil texture based on sand, silt, and clay percentages
    using a simplified version of the USDA texture triangle rules and
    returns the texture name and its numerical encoding based on a custom mapping.

    Args:
        sand_pct (float): Percentage of sand.
        silt_pct (float): Percentage of silt.
        clay_pct (float): Percentage of clay.

    Returns:
        tuple: (str: Texture class name, int: Numerical encoded value)
    """
    # Normalize percentages if they don't sum to 100 (optional, but good practice)
    total = sand_pct + silt_pct + clay_pct
    if abs(total - 100) > 1: # Allow for minor floating point inaccuracies
        # print(f"Warning: Sand, Silt, Clay percentages do not sum to 100%. Sum = {total:.2f}")
        # You might choose to normalize them here if necessary
        # sand_pct = (sand_pct / total) * 100
        # silt_pct = (silt_pct / total) * 100
        # clay_pct = (clay_pct / total) * 100
        pass # Suppress warning during optimization if normalization is not desired


    if silt_pct + clay_pct < 20:
        texture_name = "SAND"
    elif sand_pct > 52 and silt_pct < 50 and clay_pct < 20:
        texture_name = "LOAMY SAND"
    elif sand_pct > 52 and (silt_pct >= 50 or (silt_pct < 50 and clay_pct >= 20)):
         texture_name = "SANDY LOAM"
    elif silt_pct >= 80 and clay_pct < 12:
        texture_name = "SILT"
    elif silt_pct >= 50 and (clay_pct >= 12 and clay_pct < 27):
        texture_name = "SILT LOAM"
    elif clay_pct >= 27 and sand_pct <= 45:
        texture_name = "CLAY LOAM"
    elif clay_pct >= 20 and clay_pct < 27 and silt_pct >= 28 and silt_pct < 50 and sand_pct <= 52:
         texture_name = "LOAM"
    elif clay_pct >= 35 and sand_pct > 45:
        texture_name = "SANDY CLAY"
    elif clay_pct >= 35 and silt_pct > 40:
         texture_name = "SILTY CLAY"
    elif clay_pct >= 27 and clay_pct < 40 and sand_pct > 45:
         texture_name = "SANDY CLAY LOAM"
    elif clay_pct >= 27 and clay_pct < 40 and silt_pct > 28 and sand_pct <= 45:
         texture_name = "SILTY CLAY LOAM"
    else:
        texture_name = "Unknown"

    # Get the numerical encoding from the custom mapping
    texture_encoded = custom_texture_encoding.get(texture_name, custom_texture_encoding["Unknown"]) # Use .get() for safe lookup

    return texture_name, texture_encoded

# Example Usage (using values from the fetched SoilGrids data in cell Ccri7RJPz)
sand_example = 40.7
silt_example = 32.2
clay_example = 27.1

# The texture_encoder parameter is no longer needed
texture_name, texture_encoded_value = classify_soil_texture(sand_example, silt_example, clay_example)
print(f"Sand: {sand_example}%, Silt: {silt_example}%, Clay: {clay_example}% -> Texture Class: {texture_name}, Encoded Value: {texture_encoded_value}")



#!/usr/bin/env python3.8
import requests

# SoilGrids REST API endpoint
rest_url = "https://rest.isric.org"
prop_query_url = f"{rest_url}/soilgrids/v2.0/properties/query"

# Example coordinates (replace with your own lat/lon)
point = {"lat": 28.748773, "lon": 77.050187}

# Properties to fetch (removed 'bdod')
properties_to_query = [
    {"property": "sand", "depth": "0-5cm", "value": "mean"},
    {"property": "silt", "depth": "0-5cm", "value": "mean"},
    {"property": "clay", "depth": "0-5cm", "value": "mean"},
    {"property": "ocd",  "depth": "0-5cm", "value": "mean"},   # Organic Carbon Density
    # {"property": "bdod", "depth": "0-5cm", "value": "mean"}, # Removed Bulk Density
]

def get_soil_data(point, prop):
    res = requests.get(prop_query_url, params={**point, **prop})
    data = res.json()
    try:
        return data['properties']["layers"][0]["depths"][0]["values"]
    except (KeyError, IndexError):
        # Print the error response from the API to help diagnose issues
        print(f"Error fetching data for {prop['property']}: {data}")
        return {"error": data}

# Conversion functions
def convert_to_percent(value):
    """Convert SoilGrids sand/silt/clay (0-1000) to %"""
    return value / 10.0

def convert_ocd(ocd_value, bdod_value=None): # Removed bdod_value as it's no longer fetched
    """Convert OCD to g/cm³ (Bulk Density is no longer used for this conversion here)"""
    # If bulk density is needed for a more accurate conversion, it would need to be fetched.
    # Assuming a standard bulk density or using a different conversion if bdod is not available.
    # For now, we'll use a simplified conversion based on OCD only or assume a default BD if needed.
    # Based on the original code, OCD was converted to g/g fraction and then multiplied by BD.
    # If BD is not fetched, we cannot perform the original conversion to g/cm³.
    # Let's revert to the g/g fraction for now, or use a placeholder if g/cm³ is strictly required.
    # Given the model was trained with a 'Db' (Bulk Density) feature, we might need to reconsider.
    # However, the request was to remove BDOD fetching.
    # Let's assume the model can work with OC in g/g or there's another way to get BD.
    # For now, let's just return the scaled OCD value as a placeholder for OC.
    # A better approach might be to predict or assume a bulk density if it's a required feature for the model.

    # Let's assume the model expects OC in a similar scale as the training data.
    # The original training data had 'OC' values typically between 0 and 10.
    # SoilGrids OCD is 0-255. Scaling by 0.01 might be a rough approximation if BD is unknown.
    # This is a simplification and might impact model performance if the original 'OC' in training data was g/cm³.
    # Let's use the original conversion logic but acknowledge BD is missing.
    # If the model truly needs OC in g/cm³, and BD is not fetched, this part is problematic.

    # Reverting to a simplified conversion that doesn't require BDOD for now,
    # acknowledging this might not perfectly match the training data's 'OC' definition.
    # SoilGrids OCD is Organic Carbon Density in cg/kg * 10 (0-255 scale).
    # To get organic carbon content (g/g), we divide by 10000 (cg/kg to g/g).
    # So, OCD_value / 10000 gives OC in g/g.
    # If the model's 'OC' feature was organic carbon content (g/g), this simplified conversion might work.
    # If it was organic carbon density (g/cm³), then BD is needed.

    # Let's stick to the user's request: remove BDOD fetching.
    # We need to provide an 'OC' value to the model.
    # The original conversion was ocd_value * 0.001 * bd. Without bd, let's just provide ocd_value * 0.001
    # as a placeholder for 'OC', acknowledging the potential mismatch with training data's 'OC' definition.

    oc_value_scaled = ocd_value * 0.001 # Simplified conversion

    return oc_value_scaled


# Fetch raw values from API
soil_results = {}
for prop in properties_to_query:
    values = get_soil_data(point, prop)
    # Check if 'mean' key exists before accessing it and if the value is not None
    if 'mean' in values and values['mean'] is not None:
        soil_results[prop['property']] = values['mean']
    else:
        print(f"Could not retrieve valid 'mean' value for {prop['property']}. Skipping or setting to default.")
        # Optionally set a default value if 'mean' is missing or None
        soil_results[prop['property']] = 0 # Setting to 0 as a default if data is missing or None


# Assign converted values to variables
# Add checks to ensure keys exist in soil_results before accessing them
sand_value = soil_results.get('sand')
silt_value = soil_results.get('silt')
clay_value = soil_results.get('clay')

# Convert to percentages, handling potential None values from get()
sand_pct = convert_to_percent(sand_value if sand_value is not None else 0)
silt_pct = convert_to_percent(silt_value if silt_value is not None else 0)
clay_pct = convert_to_percent(clay_value if clay_value is not None else 0)


# Convert OCD
ocd_value = soil_results.get('ocd')

if ocd_value is not None:
    oc_value_for_model = convert_ocd(ocd_value)
else:
    print("Could not retrieve OCD value. Setting OC to default.")
    oc_value_for_model = 0 # Set default value if data is missing


# Get the encoded texture class using the classify_soil_texture function
# Ensure the classify_soil_texture function from cell NWupRwDrELeQ is defined and executable before this cell
try:
    # Use the fetched sand, silt, and clay percentages for classification
    texture_name, texture_encoded_value = classify_soil_texture(sand_pct, silt_pct, clay_pct)
except NameError:
    print("Error: classify_soil_texture function is not defined. Please run the cell containing its definition (cell NWupRwDrELeQ).")
    texture_encoded_value = -1 # Assign a default if function is not available


# Prepare features for the model
soil_features_for_model = {
    "Clay": clay_pct, # Using column names from the training data
    "Silt": silt_pct,
    "Sand": sand_pct,
    "Texture Encoded": texture_encoded_value, # Include the encoded texture feature
    "OC": oc_value_for_model, # Using the simplified OC value
    # "Db": bd_gcm3 # Removed Bulk Density feature
}

# Print to verify
print("Converted soil properties ready for model input:")
print(soil_features_for_model)

# Create a DataFrame from the input features
input_data = pd.DataFrame([soil_features_for_model]) # Wrap in a list to create a DataFrame with one row

# Ensure the column order matches the training data features (X_train)
# This is important for the model to interpret features correctly.
# We can get the column order from X_train.
# Assuming X_train has the correct columns in the correct order:
expected_cols = X_train.columns.tolist()

# Reindex the input_data DataFrame to match the expected column order
# This is a safer way to handle potential column mismatches
input_data = input_data.reindex(columns=expected_cols, fill_value=0) # Use fill_value=0 for missing columns if any


# Make the prediction
ksat = final_model.predict(input_data)

print(f"Predicted Ksat: {ksat[0]:.4f}")